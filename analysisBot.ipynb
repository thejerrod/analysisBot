{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06dc7907-847a-41bb-9fb8-e6fe22c83f25",
   "metadata": {},
   "source": [
    "PoC for case insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aecabb1-e79a-4f94-9e3d-d83c159c2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import io\n",
    "from faker import Faker\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae00274-ad9d-4563-b3a0-af3f903ec171",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_random_data(n_samples=100):\n",
    "    fake = Faker()\n",
    "    \n",
    "    reasons_for_contact = [\n",
    "        \"Account related\", \"General Problem\", \"Informational\", \"Other\",\n",
    "        \"Pre Sales\", \"Proactive\", \"Security Related\", \"Subscription / Licensing\"\n",
    "    ]\n",
    "    cause_codes = [\n",
    "        \"Configuration\", \"Consultant File Transfer\", \"ESRP\", \"Functional Question\", \n",
    "        \"Hardware / Hardware Error\", \"Installation\", \"Licensing\", \"Other Unlisted\", \n",
    "        \"Security / Vulnerability\", \"Software / Software Error\", \"Upgrade\"\n",
    "    ]\n",
    "    resolution_codes = [\n",
    "        \"Updated Configuration\", \"Upgraded Software / Engineering Hotfix\", \"Replaced Hardware\", \n",
    "        \"Educated Customer\", \"Updated Account\", \"Customer Resolved\", \"Applied Workaround\", \n",
    "        \"Updated License\", \"Not our issue\", \"Undetermined\", \"Unresolved\", \"Resolved by another group\", \n",
    "        \"Other Unlisted\", \"Lack of response\", \"Duplicate\", \"Bugzilla ID existing\", \n",
    "        \"Bugzilla ID New\", \"Request for Enhancement\"\n",
    "    ]\n",
    "    product_types = [\n",
    "        \"BIG-IP APM\", \"BIG-IP ASM\", \"BIG-IP LTM\", \"BIG-IP AFM\", \"BIG-IP PEM\",\n",
    "        \"BIG-IP Analytics\", \"BIG-IP Link Controller\", \"BIG-IP AAM\", \"BIG-IP DNS\"\n",
    "    ]\n",
    "    product_versions = ['v13', 'v14', 'v15', 'v16', 'v17']\n",
    "\n",
    "    dates = pd.date_range(start='2023-01-01', periods=n_samples, freq='D')\n",
    "    product_types_sample = np.random.choice(product_types, size=n_samples)\n",
    "    product_versions_sample = np.random.choice(product_versions, size=n_samples)\n",
    "    subjects = [fake.sentence(nb_words=6) for _ in range(n_samples)]\n",
    "    customer_descriptions = [fake.paragraph(nb_sentences=3) for _ in range(n_samples)]\n",
    "    engineer_descriptions = [fake.paragraph(nb_sentences=3) for _ in range(n_samples)]\n",
    "    environments = np.random.choice(['Env1', 'Env2', 'Env3'], size=n_samples)\n",
    "    root_causes = [fake.sentence(nb_words=10) for _ in range(n_samples)]\n",
    "    recommended_actions = [fake.sentence(nb_words=5) for _ in range(n_samples)]\n",
    "    additional_info = [fake.sentence(nb_words=5) for _ in range(n_samples)]\n",
    "    time_to_resolve = np.random.randint(1, 20, size=n_samples)\n",
    "    knowledge_base_articles = np.random.choice(['KB1', 'KB2', 'KB3'], size=n_samples)\n",
    "    bugzilla_ids = np.random.choice(['BZ1', 'BZ2', 'BZ3'], size=n_samples)\n",
    "    reasons = np.random.choice(reasons_for_contact, size=n_samples)\n",
    "    cause_codes_sample = np.random.choice(cause_codes, size=n_samples)\n",
    "    resolution_codes_sample = np.random.choice(resolution_codes, size=n_samples)\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Product type': product_types_sample,\n",
    "        'Product version': product_versions_sample,\n",
    "        'Subject': subjects,\n",
    "        'Customer description': customer_descriptions,\n",
    "        'Engineer description': engineer_descriptions,\n",
    "        'Environment': environments,\n",
    "        'Root Cause': root_causes,\n",
    "        'Recommended actions': recommended_actions,\n",
    "        'Additional Information': additional_info,\n",
    "        'Time to resolve': time_to_resolve,\n",
    "        'Knowledge base article if used': knowledge_base_articles,\n",
    "        'Bugzilla ID if found': bugzilla_ids,\n",
    "        'Reason for contact': reasons,\n",
    "        'Cause code': cause_codes_sample,\n",
    "        'Resolution code': resolution_codes_sample\n",
    "    })\n",
    "    return data\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = 'values.csv'\n",
    "\n",
    "# Check if the file exists and is not empty\n",
    "if os.path.exists(csv_file_path):\n",
    "    print(f\"The file '{csv_file_path}' exists.\")\n",
    "    if os.path.getsize(csv_file_path) > 0:\n",
    "        data = pd.read_csv(csv_file_path)\n",
    "        print(f\"The file '{csv_file_path}' is not empty. Data loaded from CSV file.\")\n",
    "    else:\n",
    "        data = generate_random_data()\n",
    "        print(f\"The file '{csv_file_path}' is empty. Generated random data.\")\n",
    "else:\n",
    "    data = generate_random_data()\n",
    "    print(f\"The file '{csv_file_path}' does not exist. Generated random data.\")\n",
    "\n",
    "# Manipulate the data to show recognizable trends\n",
    "# Example trend: Longer resolution times for 'Security Related'\n",
    "data.loc[data['Reason for contact'] == 'Security Related', 'Time to resolve'] = (data.loc[data['Reason for contact'] == 'Security Related', 'Time to resolve'] * 1.5).astype(int)\n",
    "\n",
    "# Example trend: Specific product version having more 'Software / Software Error' issues\n",
    "data.loc[data['Product version'] == 'v1.0', 'Root Cause'] = 'Software / Software Error'\n",
    "\n",
    "# Adding recognizable patterns to other fields\n",
    "# Example trend: Environment 'Env1' having more 'Configuration' issues\n",
    "data.loc[data['Environment'] == 'Env1', 'Root Cause'] = 'Configuration'\n",
    "\n",
    "# Example trend: Product type 'BIG-IP APM' having longer resolution times\n",
    "data.loc[data['Product type'] == 'BIG-IP APM', 'Time to resolve'] += 5\n",
    "\n",
    "print(\"Sample data with 100 examples loaded successfully.\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5950ab4-4dd2-48fc-89e8-470673e716a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration\n",
    "# Display the first few rows of the dataframe\n",
    "print(data.head())\n",
    "\n",
    "# Summary statistics\n",
    "print(data.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Data Visualization\n",
    "# Distribution of the 'Reason for contact'\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data['Reason for contact'])\n",
    "plt.title('Distribution of Reason for Contact')\n",
    "plt.xlabel('Reason for Contact')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of Numerical Variables\n",
    "numerical_columns = data.select_dtypes(include=[np.number]).columns\n",
    "data[numerical_columns].hist(figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Value Counts for Categorical Variables\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    print(f\"Value counts for {column}:\")\n",
    "    print(data[column].value_counts())\n",
    "    print()\n",
    "\n",
    "# Outlier Detection\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data['Time to resolve'])\n",
    "plt.title('Box Plot for Time to Resolve (Outlier Detection)')\n",
    "plt.xlabel('Time to Resolve')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Initial Analysis\n",
    "# Example: Group by 'Reason for contact' and calculate mean 'Time to resolve'\n",
    "analysis = data.groupby('Reason for contact')['Time to resolve'].mean()\n",
    "print(analysis)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save analysis results to a CSV file\n",
    "analysis.to_csv('initial_analysis.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14050b-338d-484e-97c6-91139f9b59a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "# Select only numerical columns for correlation analysis\n",
    "numerical_data = data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Check the selected numerical columns\n",
    "print(\"Numerical columns for correlation analysis:\")\n",
    "print(numerical_data.columns)\n",
    "\n",
    "# Ensure we have more than one numerical column for correlation analysis\n",
    "if numerical_data.shape[1] > 1:\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = numerical_data.corr()\n",
    "\n",
    "    # Plot the correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough numerical columns for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488ed22-df54-4fe1-99c0-470849bf3e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation Matrix\n",
    "# Measures linear correlation between numerical variables.\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(numerical_data.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Pearson Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e58015c-d933-40ff-bdde-e16774449aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman Correlation Matrix\n",
    "# Measures rank correlation, useful for non-linear relationships.\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(numerical_data.corr(method='spearman'), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Spearman Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064dc1f0-612e-4fd2-913e-59bd478888e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot for 'Time to resolve' by 'Reason for contact'\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Reason for contact', y='Time to resolve', data=data)\n",
    "plt.title('Box Plot of Time to Resolve by Reason for Contact')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb6a7d0-e852-4dd2-9bb7-23a4c420eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kendall Correlation Matrix\n",
    "# Another measure of rank correlation, robust to small datasets and ties.\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(numerical_data.corr(method='kendall'), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Kendall Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dab699-28b6-4024-9e6d-3bc155d0365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair Plot (Scatterplot Matrix)\n",
    "# Visualizes pairwise relationships and distributions of numerical variables.\n",
    "sns.pairplot(numerical_data)\n",
    "plt.suptitle('Pair Plot of Numerical Data', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618e80b-0b48-45bd-b6a5-68bd12db0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Heatmap with Annotations\n",
    "# Provides more detailed annotations on the heatmap.\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(numerical_data.corr(), annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5, cbar_kws={'label': 'Correlation Coefficient'})\n",
    "plt.title('Enhanced Pearson Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2283a83-b5bb-4ae4-9262-dbed9e9bea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix with Clustering\n",
    "# Groups variables based on similarity in their correlation patterns.\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Calculate the linkage\n",
    "Z = linkage(numerical_data.corr(), 'ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z, labels=numerical_data.columns, leaf_rotation=90)\n",
    "plt.title('Dendrogram of Correlation Matrix')\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaa17e4-096a-4841-8782-7bb936422014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with Categorical Data (Cramér's V) \n",
    "# Measures association between categorical variables.\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "    rcorr = r - ((r - 1) ** 2) / (n - 1)\n",
    "    kcorr = k - ((k - 1) ** 2) / (n - 1)\n",
    "    # Check to avoid division by zero\n",
    "    if min((kcorr - 1), (rcorr - 1)) == 0:\n",
    "        return np.nan\n",
    "    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
    "\n",
    "# Load the data from values.csv\n",
    "data = pd.read_csv('values.csv')\n",
    "\n",
    "# Select categorical columns\n",
    "categorical_data = data.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# Handle missing values by filling with a placeholder\n",
    "categorical_data = categorical_data.fillna('Missing')\n",
    "\n",
    "# Remove columns with only one unique value\n",
    "categorical_data = categorical_data[[col for col in categorical_data.columns if categorical_data[col].nunique() > 1]]\n",
    "\n",
    "# Initialize the correlation matrix\n",
    "categorical_corr = pd.DataFrame(index=categorical_data.columns, columns=categorical_data.columns)\n",
    "\n",
    "# Compute Cramér's V for categorical columns\n",
    "for col1 in categorical_data.columns:\n",
    "    for col2 in categorical_data.columns:\n",
    "        if col1 != col2:\n",
    "            categorical_corr.loc[col1, col2] = cramers_v(categorical_data[col1], categorical_data[col2])\n",
    "        else:\n",
    "            categorical_corr.loc[col1, col2] = 1.0  # Set the diagonal to 1.0\n",
    "\n",
    "categorical_corr = categorical_corr.astype(float)\n",
    "\n",
    "# Plot the Categorical Correlation Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(categorical_corr, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Categorical Correlation Matrix (Cramér\\'s V)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d88aa5-3ce0-4aa5-a00a-06645555fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud for 'Customer description'\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text = \" \".join(description for description in data['Customer description'])\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf51e97-a80d-4517-9163-0a28730e86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis of Customer Descriptions\n",
    "data['Sentiment'] = data['Customer description'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Visualize the distribution of sentiment polarity\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data['Sentiment'], bins=20, kde=True)\n",
    "plt.title('Distribution of Sentiment in Customer Descriptions')\n",
    "plt.xlabel('Sentiment Polarity')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136cb9b9-3a23-4992-97fa-58fbab3578b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modeling with LDA: Identify topics within customer descriptions.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Prepare the text data for topic modeling\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(data['Customer description'])\n",
    "\n",
    "# Fit the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Display the top words for each topic\n",
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    print(f\"Topic #{topic_idx}: {' '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a843b-832c-457d-9549-1b7c69507905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the length of customer descriptions\n",
    "data['Description Length'] = data['Customer description'].apply(len)\n",
    "\n",
    "# Visualize the distribution of description lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data['Description Length'], bins=20, kde=True)\n",
    "plt.title('Distribution of Customer Description Lengths')\n",
    "plt.xlabel('Description Length')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f4f844-646a-472f-a527-a4fcf062ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword Exctraction\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Extract keywords using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=20)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['Customer description'])\n",
    "keywords = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the top keywords\n",
    "print(\"Top keywords:\")\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94e7fc-145d-4b68-95ca-bb1fd167388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Convert text data to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(data['Customer description'])\n",
    "\n",
    "# Fit KMeans clustering model\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "data['Cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Cluster', data=data)\n",
    "plt.title('Distribution of Customer Description Clusters')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867668b-4226-4a23-80af-48ab6daf6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram Analysis from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def get_ngrams(text, n=2):\n",
    "    words = [word for word in text.split() if word.lower() not in ENGLISH_STOP_WORDS]\n",
    "    ngrams = zip(*[words[i:] for i in range(n)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "# Extract bigrams from customer descriptions\n",
    "bigrams = Counter()\n",
    "for description in data['Customer description']:\n",
    "    bigrams.update(get_ngrams(description, n=2))\n",
    "\n",
    "# Display the most common bigrams\n",
    "print(\"Most common bigrams:\")\n",
    "for bigram, count in bigrams.most_common(10):\n",
    "    print(f\"{bigram}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd30337b-622f-4caa-9249-f35bcc05b2ac",
   "metadata": {},
   "source": [
    "*** Spacy NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5679e1-de04-413d-a239-da9d4dea24ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "\n",
    "# Install the larger spaCy model\n",
    "# !python -m spacy download en_core_web_md\n",
    "\n",
    "# Load the spaCy model for NER and word vectors\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Load the data from values.csv\n",
    "data = pd.read_csv('values.csv')\n",
    "\n",
    "# Function to extract n-grams\n",
    "def extract_ngrams(text_series, n=2, top_n=20):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english')\n",
    "    X = vectorizer.fit_transform(text_series)\n",
    "    ngrams = vectorizer.get_feature_names_out()\n",
    "    counts = X.sum(axis=0).A1\n",
    "    ngram_freq = dict(zip(ngrams, counts))\n",
    "    sorted_ngrams = sorted(ngram_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_ngrams[:top_n]\n",
    "\n",
    "# Extract top 20 bigrams and trigrams\n",
    "top_bigrams = extract_ngrams(data['Customer description'], n=2, top_n=20)\n",
    "top_trigrams = extract_ngrams(data['Customer description'], n=3, top_n=20)\n",
    "\n",
    "# Generate patterns from top n-grams\n",
    "def generate_patterns(ngrams, label):\n",
    "    patterns = []\n",
    "    for ngram, _ in ngrams:\n",
    "        pattern = {\"label\": label, \"pattern\": ngram}\n",
    "        patterns.append(pattern)\n",
    "    return patterns\n",
    "\n",
    "# Create patterns for EntityRuler\n",
    "bigram_patterns = generate_patterns(top_bigrams, \"BIGRAM\")\n",
    "trigram_patterns = generate_patterns(top_trigrams, \"TRIGRAM\")\n",
    "patterns = bigram_patterns + trigram_patterns\n",
    "\n",
    "# Create patterns for Matcher\n",
    "def generate_matcher_patterns(ngrams):\n",
    "    matcher_patterns = {}\n",
    "    for ngram, _ in ngrams:\n",
    "        words = ngram.split()\n",
    "        pattern = [{\"LOWER\": word} for word in words]\n",
    "        matcher_patterns[ngram] = pattern\n",
    "    return matcher_patterns\n",
    "\n",
    "matcher_patterns = generate_matcher_patterns(top_bigrams + top_trigrams)\n",
    "\n",
    "# Part-of-Speech (POS) Tagging\n",
    "def pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "# Dependency Parsing\n",
    "def dependency_parsing(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.dep_, token.head.text) for token in doc]\n",
    "\n",
    "# Noun Phrase Extraction\n",
    "def extract_noun_phrases(text):\n",
    "    doc = nlp(text)\n",
    "    return [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "# Text Summarization\n",
    "def summarize_text(text, n_sentences=2):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    return ' '.join(sentences[:n_sentences])\n",
    "\n",
    "# Entity Ruler\n",
    "def entity_ruler(text, patterns):\n",
    "    if 'entity_ruler' not in nlp.pipe_names:\n",
    "        ruler = nlp.add_pipe(\"entity_ruler\", after=\"ner\")\n",
    "        ruler.add_patterns(patterns)\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Text Cleaning\n",
    "def clean_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if token.text.lower() not in STOP_WORDS and token.text not in string.punctuation]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Pattern Matching\n",
    "def pattern_matching(text, patterns):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    for pattern_id, pattern in patterns.items():\n",
    "        matcher.add(pattern_id, [pattern])\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    return [(doc[start:end].text, nlp.vocab.strings[match_id]) for match_id, start, end in matches]\n",
    "\n",
    "# Sentence Segmentation\n",
    "def sentence_segmentation(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text for sent in doc.sents]\n",
    "\n",
    "# Semantic Similarity Analysis\n",
    "def semantic_similarity(text1, text2):\n",
    "    doc1 = nlp(text1)\n",
    "    doc2 = nlp(text2)\n",
    "    return doc1.similarity(doc2)\n",
    "\n",
    "# Apply the functions to customer descriptions\n",
    "data['Entities'] = data['Customer description'].apply(lambda x: [(ent.text, ent.label_) for ent in nlp(x).ents])\n",
    "data['POS Tags'] = data['Customer description'].apply(pos_tagging)\n",
    "data['Dependency Parse'] = data['Customer description'].apply(dependency_parsing)\n",
    "data['Noun Phrases'] = data['Customer description'].apply(extract_noun_phrases)\n",
    "data['Summary'] = data['Customer description'].apply(summarize_text)\n",
    "data['Entity Ruler'] = data['Customer description'].apply(lambda x: entity_ruler(x, patterns))\n",
    "data['Cleaned Text'] = data['Customer description'].apply(clean_text)\n",
    "data['Pattern Matches'] = data['Customer description'].apply(lambda x: pattern_matching(x, matcher_patterns))\n",
    "data['Sentences'] = data['Customer description'].apply(sentence_segmentation)\n",
    "\n",
    "# Example similarity between the first two customer descriptions\n",
    "similarity = semantic_similarity(data['Customer description'].iloc[0], data['Customer description'].iloc[1])\n",
    "print(f\"Similarity between the first two customer descriptions: {similarity:.2f}\")\n",
    "\n",
    "# Display some examples\n",
    "print(\"Named Entities:\\n\", data[['Customer description', 'Entities']].head(), \"\\n\")\n",
    "print(\"POS Tags:\\n\", data[['Customer description', 'POS Tags']].head(), \"\\n\")\n",
    "print(\"Dependency Parse:\\n\", data[['Customer description', 'Dependency Parse']].head(), \"\\n\")\n",
    "print(\"Noun Phrases:\\n\", data[['Customer description', 'Noun Phrases']].head(), \"\\n\")\n",
    "print(\"Summary:\\n\", data[['Customer description', 'Summary']].head(), \"\\n\")\n",
    "print(\"Entity Ruler:\\n\", data[['Customer description', 'Entity Ruler']].head(), \"\\n\")\n",
    "print(\"Cleaned Text:\\n\", data[['Customer description', 'Cleaned Text']].head(), \"\\n\")\n",
    "print(\"Pattern Matches:\\n\", data[['Customer description', 'Pattern Matches']].head(), \"\\n\")\n",
    "print(\"Sentences:\\n\", data[['Customer description', 'Sentences']].head(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a66efa4-c1e3-42ab-a208-ba247d6a8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keywords using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=20)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['Customer description'])\n",
    "keywords = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the top keywords\n",
    "print(\"Top keywords:\")\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b2a2d-2afb-44cb-b7f3-a129fecd26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification with Machine Learning\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prepare data for text classification\n",
    "X = data['Customer description']\n",
    "y = data['Resolution code'].apply(lambda x: 1 if x == 'Resolved' else 0)  # Binary classification: Resolved vs. Unresolved\n",
    "\n",
    "# Check the distribution of the target variable\n",
    "print(y.value_counts())\n",
    "\n",
    "# Ensure there are at least two classes in the target variable\n",
    "if len(np.unique(y)) > 1:\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert text data to TF-IDF features\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train a logistic regression model\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Make predictions and evaluate the model\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "else:\n",
    "    print(\"Not enough classes in the target variable for classification.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c9c205-0628-4760-ad91-2d2656ed99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Time to resolve based on other features\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prepare the data for predictive modeling\n",
    "X = data.drop(columns=['Time to resolve', 'Date'])  # Drop the target and any non-numeric columns\n",
    "X = pd.get_dummies(X, drop_first=True)  # Convert categorical variables to dummy variables\n",
    "y = data['Time to resolve']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c81e00-1b3b-4ce2-a522-5fc17a20243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection in Text\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Convert text data to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(data['Customer description'])\n",
    "\n",
    "# Fit the Isolation Forest model\n",
    "clf = IsolationForest(random_state=42)\n",
    "data['Anomaly'] = clf.fit_predict(X)\n",
    "\n",
    "# Display the number of anomalies\n",
    "print(data['Anomaly'].value_counts())\n",
    "\n",
    "# Display some examples of anomalies\n",
    "print(data[data['Anomaly'] == -1][['Customer description', 'Anomaly']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce48e5f-348a-4f29-b8ec-bd92c15a35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis by Product Type\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Product type', y='Sentiment', data=data)\n",
    "plt.title('Sentiment Analysis by Product Type')\n",
    "plt.xlabel('Product Type')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Sentiment Analysis by Reason for Contact\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Reason for contact', y='Sentiment', data=data)\n",
    "plt.title('Sentiment Analysis by Reason for Contact')\n",
    "plt.xlabel('Reason for Contact')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b5cbf-ccf4-4c78-a641-497b7fa30f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text length between resolved and unresolved cases\n",
    "resolved_cases = data[data['Resolution code'] != 'Unresolved']\n",
    "unresolved_cases = data[data['Resolution code'] == 'Unresolved']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(resolved_cases['Description Length'], color='blue', label='Resolved', kde=True)\n",
    "sns.histplot(unresolved_cases['Description Length'], color='red', label='Unresolved', kde=True)\n",
    "plt.title('Comparison of Description Lengths between Resolved and Unresolved Cases')\n",
    "plt.xlabel('Description Length')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a4f95-c585-450b-989b-549b121b5953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of 'Resolution code'\n",
    "# Define resolution codes that are considered \"resolved\". The function needs a binary value\n",
    "resolved_codes = [\n",
    "    \"Upgraded Software / Engineering Hotfix\", \"Request for Enhancement\", \"Bugzilla ID existing\",\n",
    "    \"Updated Configuration\", \"Resolved by another group\", \"Duplicate\", \"Bugzilla ID New\",\n",
    "    \"Applied Workaround\", \"Updated License\", \"Customer Resolved\", \"Updated Account\",\n",
    "    \"Educated Customer\"\n",
    "]\n",
    "\n",
    "# Prepare data for text classification\n",
    "X = data['Customer description']\n",
    "y = data['Resolution code'].apply(lambda x: 1 if x in resolved_codes else 0)  # Binary classification: Resolved vs. Unresolved\n",
    "\n",
    "# Check the distribution of the target variable\n",
    "print(y.value_counts())\n",
    "\n",
    "# Ensure there are at least two classes in the target variable\n",
    "if len(np.unique(y)) > 1:\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert text data to TF-IDF features\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train a logistic regression model\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Make predictions and evaluate the model\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "else:\n",
    "    print(\"Not enough classes in the target variable for classification.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d20ec9-5fd0-440b-9025-f79c49c221e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Analysis\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Resample the data by month and calculate the mean 'Time to resolve'\n",
    "monthly_data = data['Time to resolve'].resample('ME').mean()\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "monthly_data.plot()\n",
    "plt.title('Average Time to Resolve Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Time to Resolve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065368d5-9b94-4039-9c5f-34610efd4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Select numerical columns for clustering\n",
    "numerical_data = data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(numerical_data)\n",
    "data['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='Time to resolve', y='Cause code', hue='Cluster', data=data, palette='viridis')\n",
    "plt.title('Clustering of Cases')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee5ef8-c98c-4620-9f2d-faee90ba7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Relationships with Knowledge Base Article\n",
    "\n",
    "# Check the distribution of knowledge base articles\n",
    "kb_article_distribution = data['Knowledge base article if used'].value_counts()\n",
    "print(\"Distribution of Knowledge Base Articles:\")\n",
    "print(kb_article_distribution)\n",
    "\n",
    "# Find relationships between KB articles and other fields\n",
    "# Group by 'Knowledge base article if used' and calculate the mean of numerical fields\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "kb_article_relationships = data.groupby('Knowledge base article if used')[numeric_columns].mean()\n",
    "print(\"\\nRelationships between KB Articles and Numerical Fields:\")\n",
    "print(kb_article_relationships)\n",
    "\n",
    "# Visualize the relationship between KB articles and 'Time to resolve'\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Knowledge base article if used', y='Time to resolve', data=data)\n",
    "plt.title('Box Plot of Time to Resolve by Knowledge Base Article')\n",
    "plt.xlabel('Knowledge Base Article')\n",
    "plt.ylabel('Time to Resolve')\n",
    "plt.show()\n",
    "\n",
    "# Analyze the relationships with categorical fields\n",
    "categorical_fields = ['Reason for contact', 'Cause code', 'Resolution code', 'Product type']\n",
    "for field in categorical_fields:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x=field, hue='Knowledge base article if used', data=data)\n",
    "    plt.title(f'Distribution of {field} by Knowledge Base Article')\n",
    "    plt.xlabel(field)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='KB Article')\n",
    "    plt.show()\n",
    "    \n",
    "# Frequency Analysis of Knowledge Base Articles\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(y='Knowledge base article if used', data=data, order=data['Knowledge base article if used'].value_counts().index)\n",
    "plt.title('Frequency of Knowledge Base Articles')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Knowledge Base Article')\n",
    "plt.show()\n",
    "\n",
    "# Resolution Effectiveness Analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Knowledge base article if used', y='Time to resolve', data=data)\n",
    "plt.title('Resolution Time by Knowledge Base Article')\n",
    "plt.xlabel('Knowledge Base Article')\n",
    "plt.ylabel('Time to Resolve')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Calculate average resolution time with and without KB articles\n",
    "with_kb = data[data['Knowledge base article if used'] != 'None']['Time to resolve'].mean()\n",
    "without_kb = data[data['Knowledge base article if used'] == 'None']['Time to resolve'].mean()\n",
    "print(f\"Average resolution time with KB: {with_kb}\")\n",
    "print(f\"Average resolution time without KB: {without_kb}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c243f77a-25c7-4a87-a33c-331e6006e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Select only numerical columns for modeling\n",
    "numerical_data = data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Check if numerical_data is empty or contains missing values\n",
    "print(numerical_data.head())\n",
    "print(numerical_data.isnull().sum())\n",
    "\n",
    "# Prepare the data\n",
    "X = numerical_data.drop('Time to resolve', axis=1, errors='ignore')\n",
    "y = numerical_data['Time to resolve']\n",
    "\n",
    "# Ensure X and y are not empty\n",
    "if X.empty or y.empty:\n",
    "    raise ValueError(\"The input data X or y is empty. Please check the data preparation steps.\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154dd31-c708-4f66-8e73-bdf7efabb668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Extract month and day from Date\n",
    "data['Month'] = data.index.month\n",
    "data['Day'] = data.index.day\n",
    "data['Day of Week'] = data.index.dayofweek\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5cc2e-987c-4538-a026-0f34d29d57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Cross-Validation Score: {-scores.mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f22a500-f575-4e1c-bd39-d5d2a506b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f'Cross-Validation Scores: {scores}')\n",
    "print(f'Mean Cross-Validation Score: {-scores.mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8ed96-9d6b-464d-a962-3a8ad46efc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c75ff-79b4-48ba-9a9e-350ad4d4fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model for NER and word vectors\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Load the data from values.csv\n",
    "data = pd.read_csv('values.csv')\n",
    "\n",
    "# Data Preparation\n",
    "X = data.drop(columns=['Time to resolve', 'Date'])  # Drop the target and any non-numeric columns\n",
    "X = pd.get_dummies(X, drop_first=True)  # Convert categorical variables to dummy variables\n",
    "feature_names = X.columns  # Capture feature names before transforming to sparse matrix\n",
    "y = data['Time to resolve']\n",
    "\n",
    "# Predictive Modeling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Feature Importance\n",
    "importances = model.feature_importances_\n",
    "feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances.head(10))\n",
    "plt.title('Top 10 Important Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d38262-e9a2-45f7-a520-9645f56870dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Fit the model\n",
    "iso_forest = IsolationForest(contamination=0.1)\n",
    "data['Anomaly'] = iso_forest.fit_predict(numerical_data)\n",
    "\n",
    "# Plot the anomalies\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='Time to resolve', y='Cause code', hue='Anomaly', data=data, palette='viridis')\n",
    "plt.title('Anomaly Detection')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f989b-f87e-481e-a7d5-17505494e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Apply sentiment analysis on 'Customer description'\n",
    "data['Sentiment'] = data['Customer description'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Plot sentiment scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data['Sentiment'], bins=10, kde=True)\n",
    "plt.title('Sentiment Analysis of Customer Descriptions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d8cd3-2c98-475b-956c-8ffa842c378d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# Initialize Plotly with a different renderer if needed\n",
    "pio.renderers.default = 'notebook'\n",
    "\n",
    "# Reset the index to ensure Plotly can interpret the DataFrame correctly\n",
    "data_reset = data.reset_index()\n",
    "\n",
    "# Print the data to ensure it's correctly formatted\n",
    "print(data_reset.head())\n",
    "\n",
    "# Create an interactive scatter plot\n",
    "fig = px.scatter(data_reset, x='Time to resolve', y='Cause code', color='Reason for contact',\n",
    "                 title='Interactive Scatter Plot of Time to Resolve by Cause Code')\n",
    "\n",
    "# Display the figure using show method with different renderer options\n",
    "fig.show(renderer='notebook')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4c3c79-09e6-4dd9-8935-633635f02f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pair plot to visualize relationships between variables\n",
    "sns.pairplot(data_reset, hue='Reason for contact')\n",
    "plt.suptitle('Pair Plot of Key Variables', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78bb70-d127-4e98-a0df-d2e16ec83322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Heatmap of missing values\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(data_reset.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Heatmap of Missing Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d53ea8-8fa9-49d4-a566-b62b5d54fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Violin plot to visualize the distribution of 'Time to resolve' by 'Reason for contact'\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(x='Reason for contact', y='Time to resolve', data=data_reset, palette='muted')\n",
    "plt.title('Violin Plot of Time to Resolve by Reason for Contact')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0e233-a41b-443f-93e0-0de6037ba835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode categorical columns\n",
    "le_cause_code = LabelEncoder()\n",
    "le_reason_for_contact = LabelEncoder()\n",
    "\n",
    "data_reset['Cause code'] = le_cause_code.fit_transform(data_reset['Cause code'])\n",
    "data_reset['Reason for contact'] = le_reason_for_contact.fit_transform(data_reset['Reason for contact'])\n",
    "\n",
    "# Train a decision tree classifier\n",
    "X = data_reset[['Cause code', 'Time to resolve']]\n",
    "y = data_reset['Reason for contact']\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(clf, filled=True, feature_names=['Cause code', 'Time to resolve'], class_names=le_reason_for_contact.classes_)\n",
    "plt.title('Decision Tree Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c428f-a284-4a47-97f4-2bf0abce8c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Prepare the text data\n",
    "text_data = data_reset['Customer description'].tolist()\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_text = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Fit LDA model\n",
    "lda = LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "lda.fit(X_text)\n",
    "\n",
    "# Display the top words for each topic\n",
    "num_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic #{topic_idx + 1}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-num_words - 1:-1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261cb8d-23d4-4020-852f-497e7fae0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode 'Cause code' as numeric values for plotting\n",
    "le_cause_code = LabelEncoder()\n",
    "data_reset['Cause code'] = le_cause_code.fit_transform(data_reset['Cause code'])\n",
    "\n",
    "# Prepare the data\n",
    "source = ColumnDataSource(data_reset)\n",
    "\n",
    "# Create a Bokeh plot\n",
    "output_notebook()\n",
    "p = figure(title='Interactive Plot of Time to Resolve by Cause Code', x_axis_label='Time to resolve', y_axis_label='Cause code')\n",
    "\n",
    "# Use scatter method instead of circle to avoid deprecation warning\n",
    "p.scatter('Time to resolve', 'Cause code', source=source, size=10, color='navy', alpha=0.5)\n",
    "\n",
    "# Show the plot\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c2727-a49e-47ff-8301-a059df1261ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clustering with t-SNE\n",
    "# Applies t-SNE for dimensionality reduction and visualizes the clusters.\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "# Prepare the data for t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(numerical_data)\n",
    "\n",
    "# Create a DataFrame for the t-SNE results\n",
    "tsne_df = pd.DataFrame(tsne_results, columns=['TSNE1', 'TSNE2'])\n",
    "tsne_df['Reason for contact'] = data_reset['Reason for contact']\n",
    "\n",
    "# Plot the t-SNE results\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='Reason for contact', data=tsne_df, palette='viridis')\n",
    "plt.title('t-SNE Clustering of Cases')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c993538-c862-40d1-97ce-988791b6bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare the data for parallel coordinates plot\n",
    "parallel_data = data[['Reason for contact', 'Time to resolve', 'Cause code', 'Product type']].copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "le_reason = LabelEncoder()\n",
    "le_product = LabelEncoder()\n",
    "le_cause_code = LabelEncoder()\n",
    "\n",
    "parallel_data['Reason for contact'] = le_reason.fit_transform(parallel_data['Reason for contact'])\n",
    "parallel_data['Product type'] = le_product.fit_transform(parallel_data['Product type'])\n",
    "parallel_data['Cause code'] = le_cause_code.fit_transform(parallel_data['Cause code'])\n",
    "\n",
    "# Plot the parallel coordinates plot\n",
    "plt.figure(figsize=(15, 7))\n",
    "parallel_coordinates(parallel_data, 'Reason for contact', colormap=plt.get_cmap(\"Set2\"))\n",
    "plt.title('Parallel Coordinates Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9188d0-08e3-4293-b53e-32fb0fe5a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Prepare the data for radial bar chart\n",
    "radial_data = data.groupby('Product type')['Time to resolve'].mean().reset_index()\n",
    "\n",
    "# Print the radial_data to debug\n",
    "print(radial_data)\n",
    "\n",
    "# Create a radial bar chart\n",
    "fig = go.Figure(go.Barpolar(\n",
    "    r=radial_data['Time to resolve'],\n",
    "    theta=[f\"Type {i+1}\" for i in range(len(radial_data))],  # Ensure theta is categorical\n",
    "    marker_color=['#ff6361', '#bc5090', '#ffa600'],\n",
    "    marker_line_color=\"black\",\n",
    "    marker_line_width=2,\n",
    "    opacity=0.8\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Radial Bar Chart of Time to Resolve by Product Type',\n",
    "    polar=dict(\n",
    "        radialaxis=dict(showticklabels=True, ticks=''),\n",
    "    ),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471acba8-9c86-428c-ad06-3859b4420df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA example\n",
    "# PCA is useful for dimensionality reduction and visualization of high-dimensional data.\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data for PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_results = pca.fit_transform(numerical_data)\n",
    "\n",
    "# Create a DataFrame for the PCA results\n",
    "pca_df = pd.DataFrame(pca_results, columns=['PCA1', 'PCA2'])\n",
    "pca_df['Reason for contact'] = data['Reason for contact']\n",
    "\n",
    "# Plot the PCA results\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='Reason for contact', data=pca_df)\n",
    "plt.title('PCA of Cases')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ef72d-e8ee-45bd-89ac-060f61a2fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing relationships with Bugzilla IDs\n",
    "\n",
    "# Frequency Analysis of Bugzilla IDs\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(y='Bugzilla ID if found', data=data, order=data['Bugzilla ID if found'].value_counts().index)\n",
    "plt.title('Frequency of Bugzilla IDs')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Bugzilla ID')\n",
    "plt.show()\n",
    "\n",
    "# Resolution Effectiveness Analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Bugzilla ID if found', y='Time to resolve', data=data)\n",
    "plt.title('Resolution Time by Bugzilla ID')\n",
    "plt.xlabel('Bugzilla ID')\n",
    "plt.ylabel('Time to Resolve')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Calculate average resolution time with and without Bugzilla IDs\n",
    "with_bz = data[data['Bugzilla ID if found'] != 'None']['Time to resolve'].mean()\n",
    "without_bz = data[data['Bugzilla ID if found'] == 'None']['Time to resolve'].mean()\n",
    "print(f\"Average resolution time with Bugzilla ID: {with_bz}\")\n",
    "print(f\"Average resolution time without Bugzilla ID: {without_bz}\")\n",
    "\n",
    "# Categorical Relationships with Bugzilla IDs\n",
    "categorical_fields = ['Reason for contact', 'Cause code', 'Resolution code', 'Product type']\n",
    "for field in categorical_fields:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x=field, hue='Bugzilla ID if found', data=data)\n",
    "    plt.title(f'Distribution of {field} by Bugzilla ID')\n",
    "    plt.xlabel(field)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Bugzilla ID')\n",
    "    plt.show()\n",
    "\n",
    "# Bugzilla ID and Product Type\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Product type', hue='Bugzilla ID if found', data=data)\n",
    "plt.title('Distribution of Product Type by Bugzilla ID')\n",
    "plt.xlabel('Product Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Bugzilla ID')\n",
    "plt.show()\n",
    "\n",
    "# Bugzilla ID and Resolution Code\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Resolution code', hue='Bugzilla ID if found', data=data)\n",
    "plt.title('Distribution of Resolution Code by Bugzilla ID')\n",
    "plt.xlabel('Resolution Code')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Bugzilla ID')\n",
    "plt.show()\n",
    "\n",
    "# Bugzilla ID and Cause Code\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Cause code', hue='Bugzilla ID if found', data=data)\n",
    "plt.title('Distribution of Cause Code by Bugzilla ID')\n",
    "plt.xlabel('Cause Code')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Bugzilla ID')\n",
    "plt.show()\n",
    "\n",
    "# Time to Resolve by Bugzilla ID\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Bugzilla ID if found', y='Time to resolve', data=data)\n",
    "plt.title('Time to Resolve by Bugzilla ID')\n",
    "plt.xlabel('Bugzilla ID')\n",
    "plt.ylabel('Time to Resolve')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Calculate average resolution time for each Bugzilla ID\n",
    "avg_res_time_bz = data.groupby('Bugzilla ID if found')['Time to resolve'].mean().reset_index()\n",
    "avg_res_time_bz.columns = ['Bugzilla ID', 'Average Time to Resolve']\n",
    "display(avg_res_time_bz)\n",
    "\n",
    "# Cluster Analysis with KMeans\n",
    "# Select features for clustering\n",
    "features = ['Time to resolve', 'Cause code', 'Resolution code']\n",
    "data_cluster = data[features].copy()\n",
    "\n",
    "# Encode categorical features\n",
    "for column in features:\n",
    "    if data_cluster[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        data_cluster[column] = le.fit_transform(data_cluster[column])\n",
    "\n",
    "# Perform KMeans clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "data['Cluster'] = kmeans.fit_predict(data_cluster)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='Time to resolve', y='Cause code', hue='Cluster', data=data, palette='Set2')\n",
    "plt.title('Cluster Analysis of Bugzilla IDs')\n",
    "plt.xlabel('Time to Resolve')\n",
    "plt.ylabel('Cause Code')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Sentiment Analysis of Customer Descriptions for Bugzilla IDs\n",
    "data['Sentiment'] = data['Customer description'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Bugzilla ID if found', y='Sentiment', data=data)\n",
    "plt.title('Sentiment of Customer Descriptions by Bugzilla ID')\n",
    "plt.xlabel('Bugzilla ID')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Simulating a Customer Feedback column for this analysis\n",
    "np.random.seed(42)\n",
    "customer_feedback = np.random.choice(['Positive', 'Neutral', 'Negative'], size=data.shape[0])\n",
    "data.loc[:, 'Customer Feedback'] = customer_feedback\n",
    "\n",
    "# Customer Feedback by Bugzilla ID\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Customer Feedback', hue='Bugzilla ID if found', data=data)\n",
    "plt.title('Customer Feedback by Bugzilla ID')\n",
    "plt.xlabel('Customer Feedback')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Bugzilla ID')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Decision Tree to Understand Factors Leading to Bugzilla ID\n",
    "X = data[['Time to resolve', 'Cause code', 'Resolution code', 'Product type']]\n",
    "y = data['Bugzilla ID if found']\n",
    "\n",
    "# Encode categorical features\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        X[column] = le.fit_transform(X[column])\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(clf, filled=True, feature_names=X.columns, class_names=clf.classes_)\n",
    "plt.title('Decision Tree to Understand Factors Leading to Bugzilla ID')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5548c-4168-4ede-bcf8-eb05afdae572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
